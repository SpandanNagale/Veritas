services:
  # 1. The Brain (Local LLM Server)
  # We use Ollama here as a service because it's easier to set up than vLLM initially, 
  # but we treat it as an external API (Production style).
  llm-server:
    image: ollama/ollama:latest
    container_name: veritas_llm
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: always

  # 2. The Knowledge Graph (Structured Data)
  graph-db:
    image: neo4j:5.15.0
    container_name: veritas_graph
    ports:
      - "7474:7474" # Browser UI
      - "7687:7687" # Bolt Protocol
    environment:
      NEO4J_AUTH: neo4j/veritas_password
    volumes:
      - ./neo4j_data:/data

  # 3. The Vector Store (Unstructured Data)
  vector-db:
    image: chromadb/chroma:latest
    container_name: veritas_vector
    ports:
      - "8000:8000"
    volumes:
      - ./chroma_data:/chroma/chroma

  # 4. Your API (The FastAPI Application)
  api:
    build: .
    container_name: veritas_api
    ports:
      - "8080:8080"
    volumes:
      - .:/app
    depends_on:
      - llm-server
      - graph-db
      - vector-db
    environment:
      - NEO4J_URI=bolt://graph-db:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=veritas_password
      - OLLAMA_BASE_URL=http://llm-server:11434